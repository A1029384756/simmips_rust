use crate::lex_parse::token::*;

fn push_str(string: &mut String, ln: &u32, tk_list: &mut TokenList) -> () {
    if !string.is_empty() {
        tk_list.push(Token::new_token(TokenType::STRING, *ln, string.to_string()));
        string.clear()
    }
}

pub fn tokenize(in_str: &str) -> TokenList {
    let mut line_number: u32 = 0;
    let mut tokens: TokenList = TokenList::default();

    in_str.lines().for_each(|line| {
        line_number += 1;
        let line = line.split('#').nth(0).unwrap();
        let mut tmp: String = String::default();
        let mut in_paren: bool = false;

        let mut iter = line.chars().into_iter();
        while let Some(char) = iter.next() {
            match char {
                '=' => {
                    push_str(&mut tmp, &line_number, &mut tokens);
                    tokens.push(Token::new_empty_token(TokenType::EQUAL, line_number));
                }
                ' ' => {
                    iter.next();
                }
                '"' => {
                    push_str(&mut tmp, &line_number, &mut tokens);
                    tokens.push(Token::new_empty_token(TokenType::STRINGDELIM, line_number));

                    'quote_string: while {
                        let next_char = iter.next().unwrap();
                        if next_char == '\n' {
                            tokens.push(Token::new_token(
                                TokenType::ERROR,
                                line_number,
                                format!("Error: unmatched \" on line {}", line_number),
                            ));
                            tmp.clear();
                            break 'quote_string;
                        }
                        tmp.push(next_char);

                        next_char != '"'
                    } {}
                    push_str(&mut tmp, &line_number, &mut tokens);
                    tokens.push(Token::new_empty_token(TokenType::STRINGDELIM, line_number));
                }
                '(' => {
                    push_str(&mut tmp, &line_number, &mut tokens);
                    tokens.push(Token::new_empty_token(TokenType::OPENPAREN, line_number));
                    in_paren = true;
                }
                ')' => {
                    if !in_paren {
                        tokens.push(Token::new_token(
                            TokenType::ERROR,
                            line_number,
                            format!("Error: unmatched \" on line {}", line_number),
                        ));
                        tmp.clear();
                    }

                    push_str(&mut tmp, &line_number, &mut tokens);
                    tokens.push(Token::new_empty_token(TokenType::CLOSEPAREN, line_number));
                    in_paren = false;
                }
                ',' => {
                    push_str(&mut tmp, &line_number, &mut tokens);
                    tokens.push(Token::new_empty_token(TokenType::SEP, line_number));
                }
                _ => tmp.push(char),
            }
        }

        if line.len() != 0 {
            tokens.push(Token::new_empty_token(TokenType::EOL, line_number));
            return;
        }
    });

    tokens
}

#[test]
fn tokenize_test() {
    let data: &str = "
        .data
        LENGTH = 10
array:  .space LENGTH
str:    .asciiz the (end)
        .text
main:  lw $t0, array
       lw $t1, ($t0)
    unimplemented!();
    ";

    println!("{:?}", tokenize(data));
}
